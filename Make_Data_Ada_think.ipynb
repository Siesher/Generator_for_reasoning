{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "\n",
    "# --- НАСТРОЙКИ ---\n",
    "# ==============================================================================\n",
    "# Эти параметры позволяют легко настраивать поведение скрипта.\n",
    "\n",
    "# 1. Ваши API ключи Cerebras\n",
    "# Замените на реальные ключи! Получить их можно здесь: https://cloud.cerebras.ai/platform/org_.../apikeys\n",
    "# Рекомендуется использовать переменные окружения для безопасности, например:\n",
    "# export CEREBRAS_API_KEY_1=\"cbkey_xxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "API_KEYS = []\n",
    "\n",
    "# 2. Настройки модели-учителя Cerebras\n",
    "MODEL_NAME_TEACHER = \"qwen-3-235b-a22b\"\n",
    "\n",
    "# 3. Параметры параллелизма\n",
    "MAX_WORKERS = 4 # Количество одновременных запросов к API. Зависит от лимитов API и вашей пропускной способности.\n",
    "\n",
    "# 4. Настройки повторных попыток при ошибках API\n",
    "MAX_RETRIES_PER_KEY = 3 # Максимальное количество попыток с одним ключом до его смены\n",
    "RETRY_DELAY = 5         # Пауза в секундах между повторными попытками\n",
    "\n",
    "# 5. Параметры генерации датасета\n",
    "NUM_UNIQUE_QUESTIONS_TO_GENERATE = 400 # Общее количество уникальных \"тем\" вопросов для генерации.\n",
    "QUESTIONS_BATCH_SIZE = 3               # Количество вопросов, генерируемых за один API-запрос на первом этапе.\n",
    "# Сколько различных стратегий рассуждения мы хотим получить для КАЖДОГО уникального вопроса.\n",
    "# Если установлено 3, и у нас 3 стратегии (CoT, PoT, SkipThinking), то для каждого вопроса будет по 1 примеру каждой стратегии.\n",
    "NUM_REASONING_STRATEGIES_PER_QUESTION = 6 \n",
    "\n",
    "# 6. Параметры рассуждений\n",
    "WORDS_IN_REASONING = \"100-200\" # Ожидаемый объем \"мыслей\" (chain_of_thought) в словах для CoT/PoT.\n",
    "WORDS_IN_CHUNK = \"30-50\"       # Ожидаемый объем слов в каждом \"блоке мысли\" для Skip-Thinking.\n",
    "\n",
    "# 7. Файл вывода\n",
    "OUTPUT_DATASET_FILE = \"qwen3_adaptive_reasoning_dataset.jsonl\"\n",
    "\n",
    "# --- ПРОМПТЫ ---\n",
    "# ==============================================================================\n",
    "# Здесь определены промпты для различных этапов и типов рассуждений.\n",
    "\n",
    "# Промпт для генерации начальных вопросов\n",
    "QUESTION_GENERATOR_PROMPT = \"\"\"\n",
    "Твоя задача — придумать {count} уникальных и сложных вопросов на русском языке, требующих многошаговых рассуждений для ответа.\n",
    "Вопросы должны быть из следующей области: {topic}.\n",
    "Если это применимо к данной теме, сгенерируй несколько вопросов, которые повторяют одну и ту же базовую идею, но используют **разные числа, условия, контекст или параметры**, чтобы каждый вопрос оставался уникальным, но проверял схожий тип рассуждений.\n",
    "\n",
    "Верни ответ ТОЛЬКО в виде валидного JSON-массива строк. Не добавляй никаких других пояснений или текста до и после JSON.\n",
    "\n",
    "Пример:\n",
    "[\"Каким образом можно оценить примерный вес облака?\", \"Если бы у тени был вес, что бы повлияло на его изменение в течение дня?\", \"Если в саду 10 яблонь, и каждая дает по 20 кг яблок, сколько всего яблок будет собрано?\", \"Если в саду 15 яблонь, и каждая дает по 15 кг яблок, сколько всего яблок будет собрано?\"]\n",
    "\n",
    "Твой JSON-массив с {count} вопросами:\n",
    "\"\"\"\n",
    "\n",
    "# Промпт для генерации Chain-of-Thought (CoT) рассуждений\n",
    "REASONING_SOLVER_PROMPT_COT = \"\"\"\n",
    "Ты — экспертная система, способная к глубоким пошаговым рассуждениям. Твоя цель — решить поставленную задачу, подробно объяснив свой мыслительный процесс, используя только текстовые рассуждения (Chain-of-Thought).\n",
    "Ты должен сгенерировать ответ в формате JSON.\n",
    "\n",
    "ФОРМАТ ВЫВОДА:\n",
    "Ответ должен быть валидным JSON-объектом со следующими ключами: \"question\", \"chain_of_thought\", \"answer\", \"thought_type\".\n",
    "\n",
    "1.  **`chain_of_thought`**: Здесь ты должен продемонстрировать свой мыслительный процесс. Структурируй его логически, шаг за шагом. Начни с анализа вопроса, затем опиши план решения и последовательно изложи свои рассуждения. Весь текст должен быть на русском языке. Целевая длина — примерно {WORDS_IN_REASONING} слов.\n",
    "2.  **`answer`**: Здесь укажи ТОЛЬКО конечный, четкий и сжатый ответ без лишних пояснений.\n",
    "3.  **`thought_type`**: Всегда устанавливай значение \"CoT\" для этого типа рассуждения.\n",
    "\n",
    "Вопрос для решения:\n",
    "{question}\n",
    "\n",
    "Твой JSON-ответ:\n",
    "\"\"\"\n",
    "\n",
    "# Промпт для генерации Program-of-Thought (PoT) рассуждений\n",
    "REASONING_SOLVER_PROMPT_POT = \"\"\"\n",
    "Ты — экспертная система, способная к глубоким пошаговым рассуждениям. Твоя цель — решить поставленную задачу, подробно объяснив свой мыслительный процесс, используя программный код (Python) для промежуточных вычислений и проверки.\n",
    "Ты должен сгенерировать ответ в формате JSON.\n",
    "\n",
    "ФОРМАТ ВЫВОДА:\n",
    "Ответ должен быть валидным JSON-объектом со следующими ключами: \"question\", \"chain_of_thought\", \"answer\", \"thought_type\".\n",
    "\n",
    "1.  **`chain_of_thought`**: Здесь ты должен продемонстрировать свой мыслительный процесс. Структурируй его логически, шаг за шагом. Начни с анализа вопроса, затем опиши план решения, затем последовательно изложи свои рассуждения, **включая блоки кода Python для выполнения вычислений**. Используй Markdown для выделения кода. Весь текст должен быть на русском языке. Целевая длина — примерно {WORDS_IN_REASONING} слов.\n",
    "2.  **`answer`**: Здесь укажи ТОЛЬКО конечный, четкий и сжатый ответ без лишних пояснений.\n",
    "3.  **`thought_type`**: Всегда устанавливай значение \"PoT\" для этого типа рассуждения.\n",
    "\n",
    "Пример `chain_of_thought` для задачи: \"Сколько будет 5 * 8 + 3?\"\n",
    "\"Для решения этой задачи я сначала выполню умножение, а затем сложение.\n",
    "```python\n",
    "result = 5 * 8\n",
    "```\n",
    "Полученный результат умножения равен 40. Теперь добавлю 3:\n",
    "```python\n",
    "final_result = 40 + 3\n",
    "```\n",
    "Итоговый результат - 43.\"\n",
    "\n",
    "Вопрос для решения:\n",
    "{question}\n",
    "\n",
    "Твой JSON-ответ:\n",
    "\"\"\"\n",
    "\n",
    "# Промпт для генерации Skip-Thinking рассуждений (с блоками)\n",
    "REASONING_SOLVER_PROMPT_SKIP_THINKING = \"\"\"\n",
    "Ты — экспертная система, способная к глубоким пошаговым рассуждениям. Твоя цель — решить поставленную задачу, подробно объяснив свой мыслительный процесс, разбивая его на логические, атомарные \"блоки мыслей\".\n",
    "Каждый блок должен быть заключен в специальные теги: <chunk_start> и <chunk_end>.\n",
    "В конце каждого блока, после <chunk_end>, ты также должен указать один из следующих маркеров:\n",
    "- <continue_thinking>: если для решения задачи требуется дальнейшее рассуждение.\n",
    "- <end_of_thought>: если ты считаешь, что после этого блока рассуждений можно сразу перейти к финальному ответу.\n",
    "\n",
    "Ты должен сгенерировать ответ в формате JSON.\n",
    "\n",
    "ФОРМАТ ВЫВОДА:\n",
    "Ответ должен быть валидным JSON-объектом со следующими ключами: \"question\", \"chain_of_thought\", \"answer\", \"thought_type\".\n",
    "\n",
    "1.  **`chain_of_thought`**: Здесь ты должен продемонстрировать свой мыслительный процесс, разделенный на блоки. Пример:\n",
    "    <chunk_start>\n",
    "    [Шаг 1 рассуждения. Длина примерно {WORDS_IN_CHUNK} слов.]\n",
    "    </chunk_end><continue_thinking>\n",
    "    <chunk_start>\n",
    "    [Шаг 2 рассуждения. Длина примерно {WORDS_IN_CHUNK} слов.]\n",
    "    </chunk_end><end_of_thought>\n",
    "    Или, для простой задачи:\n",
    "    <chunk_start>\n",
    "    [Единственный блок рассуждения. Длина примерно {WORDS_IN_CHUNK} слов.]\n",
    "    </chunk_end><end_of_thought>\n",
    "    Весь текст должен быть на русском языке.\n",
    "2.  **`answer`**: Здесь укажи ТОЛЬКО конечный, четкий и сжатый ответ без лишних пояснений.\n",
    "3.  **`thought_type`**: Всегда устанавливай значение \"SkipThinking\" для этого типа рассуждения.\n",
    "\n",
    "Вопрос для решения:\n",
    "{question}\n",
    "\n",
    "Твой JSON-ответ:\n",
    "\"\"\"\n",
    "\n",
    "# Список всех доступных стратегий рассуждения и их промптов\n",
    "REASONING_STRATEGIES = {\n",
    "    \"CoT\": REASONING_SOLVER_PROMPT_COT,\n",
    "    \"PoT\": REASONING_SOLVER_PROMPT_POT,\n",
    "    \"SkipThinking\": REASONING_SOLVER_PROMPT_SKIP_THINKING,\n",
    "}\n",
    "\n",
    "# Темы для генерации вопросов\n",
    "QUESTION_TOPICS = [\n",
    "    \"логические загадки\", \"простая математика\", \"бытовые задачи на логику\",\n",
    "    \"задачи на пространственное мышление\", \"физические парадоксы\", \"задачи на оценку\",\n",
    "    \"алгоритмические задачи\", \"экологические задачи\", \"исторические гипотезы\",\n",
    "    \"экономические сценарии\", \"задачи на вероятность\", \"научные концепции\",\n",
    "    \"геометрические задачи\", \"финансовые расчеты\", \"задачи на время\",\n",
    "    \"криптография (основы)\", \"анализ данных (концепции)\", \"биологические процессы\"\n",
    "]\n",
    "\n",
    "# --- ГЛОБАЛЬНЫЕ ПЕРЕМЕННЫЕ И УПРАВЛЕНИЕ КЛИЕНТОМ ---\n",
    "# ==============================================================================\n",
    "\n",
    "current_key_index = 0\n",
    "cerebras_client = None\n",
    "\n",
    "def get_client():\n",
    "    \"\"\"Инициализирует или обновляет клиент Cerebras с текущим API-ключом.\"\"\"\n",
    "    global cerebras_client\n",
    "    if not API_KEYS or not API_KEYS[0].strip():\n",
    "        raise ValueError(\"API ключи Cerebras не заданы в списке API_KEYS. Пожалуйста, добавьте их.\")\n",
    "    \n",
    "    api_key = API_KEYS[current_key_index]\n",
    "    if not api_key.strip() or \"cbkey_xxxxxxxx\" in api_key: # Проверка на пустой или плейсхолдер\n",
    "        raise ValueError(f\"API ключ Cerebras #{current_key_index + 1} недействителен или не задан. Пожалуйста, замените его.\")\n",
    "    \n",
    "    cerebras_client = Cerebras(api_key=api_key)\n",
    "    return cerebras_client\n",
    "\n",
    "def switch_to_next_key():\n",
    "    \"\"\"Переключается на следующий API-ключ в списке, если доступны.\"\"\"\n",
    "    global current_key_index\n",
    "    current_key_index += 1\n",
    "    if current_key_index < len(API_KEYS):\n",
    "        tqdm.write(f\"⚠️ [INFO] Лимит для ключа исчерпан или произошла ошибка. Переключаюсь на ключ #{current_key_index + 1}.\")\n",
    "        get_client() # Инициализируем клиент с новым ключом\n",
    "        return True\n",
    "    else:\n",
    "        tqdm.write(\"❌ [ERROR] Все API ключи Cerebras исчерпаны. Невозможно продолжить.\")\n",
    "        return False\n",
    "        \n",
    "def safe_json_parse(response_content: str) -> dict | list | None:\n",
    "    \"\"\"\n",
    "    Безопасно извлекает JSON из строки, даже если он окружен другим текстом.\n",
    "    Ищет первый '{' или '[' и последний '}' или ']'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        first_brace_idx = response_content.find('{')\n",
    "        first_bracket_idx = response_content.find('[')\n",
    "        \n",
    "        start_pos = -1\n",
    "        if first_brace_idx != -1 and first_bracket_idx != -1:\n",
    "            start_pos = min(first_brace_idx, first_bracket_idx)\n",
    "        elif first_brace_idx != -1:\n",
    "            start_pos = first_brace_idx\n",
    "        elif first_bracket_idx != -1:\n",
    "            start_pos = first_bracket_idx\n",
    "        else:\n",
    "            return None # JSON не найден\n",
    "\n",
    "        if start_pos == -1: return None \n",
    "\n",
    "        end_char = '}' if response_content[start_pos] == '{' else ']'\n",
    "        end_pos = response_content.rfind(end_char)\n",
    "        \n",
    "        if end_pos == -1 or end_pos < start_pos: return None \n",
    "        \n",
    "        json_str = response_content[start_pos : end_pos + 1]\n",
    "        return json.loads(json_str)\n",
    "        \n",
    "    except (json.JSONDecodeError, IndexError) as e:\n",
    "        tqdm.write(f\"‼️ [WARN] Не удалось декодировать JSON. Ошибка: {e}\\nОтвет модели (начало): {response_content[:200]}...\")\n",
    "        return None\n",
    "\n",
    "# --- ЯДРО ЛОГИКИ: ВЫПОЛНЕНИЕ ЗАПРОСОВ ---\n",
    "# ==============================================================================\n",
    "\n",
    "def execute_api_call(prompt: str, task_description: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Выполняет API-запрос к Cerebras с автопереключением ключей и повторными попытками.\n",
    "    \"\"\"\n",
    "    global cerebras_client\n",
    "    \n",
    "    current_attempt_with_key = 0\n",
    "    while current_key_index < len(API_KEYS):\n",
    "        if current_attempt_with_key >= MAX_RETRIES_PER_KEY:\n",
    "            if not switch_to_next_key():\n",
    "                return None \n",
    "            current_attempt_with_key = 0 \n",
    "        \n",
    "        try:\n",
    "            chat_completion = cerebras_client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=model_name,\n",
    "                temperature=0.7, \n",
    "                max_tokens=2048, \n",
    "            )\n",
    "            return chat_completion.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            current_attempt_with_key += 1\n",
    "            tqdm.write(f\"‼️ [WARN] Ошибка '{task_description}' (ключ #{current_key_index + 1}, попытка {current_attempt_with_key}/{MAX_RETRIES_PER_KEY}): {e}\")\n",
    "            time.sleep(RETRY_DELAY) \n",
    "        \n",
    "    return None \n",
    "\n",
    "def generate_questions_batch(count: int, topic: str) -> list[str]:\n",
    "    \"\"\"Генерирует партию уникальных вопросов по заданной теме.\"\"\"\n",
    "    prompt = QUESTION_GENERATOR_PROMPT.format(count=count, topic=topic)\n",
    "    response_content = execute_api_call(prompt, f\"Генерация вопросов по теме '{topic}'\", MODEL_NAME_TEACHER)\n",
    "    \n",
    "    if not response_content:\n",
    "        return []\n",
    "    \n",
    "    questions = safe_json_parse(response_content)\n",
    "    return [q.strip() for q in questions if isinstance(q, str) and q.strip()] if isinstance(questions, list) else []\n",
    "\n",
    "def generate_reasoning_example(question: str, strategy_type: str, prompt_template: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Генерирует рассуждения и ответ для одного вопроса, используя заданную стратегию.\n",
    "    Включает метаданные thought_type для дистилляции Adaptive Thinking.\n",
    "    \"\"\"\n",
    "    prompt_format_args = {\n",
    "        \"question\": question,\n",
    "        \"WORDS_IN_REASONING\": WORDS_IN_REASONING\n",
    "    }\n",
    "    if strategy_type == \"SkipThinking\":\n",
    "        prompt_format_args[\"WORDS_IN_CHUNK\"] = WORDS_IN_CHUNK\n",
    "\n",
    "    prompt = prompt_template.format(**prompt_format_args)\n",
    "    response_content = execute_api_call(prompt, f\"Генерация {strategy_type} для вопроса: '{question[:50]}...'\", MODEL_NAME_TEACHER)\n",
    "\n",
    "    if not response_content:\n",
    "        return None\n",
    "        \n",
    "    data_from_model = safe_json_parse(response_content)\n",
    "    \n",
    "    required_keys = [\"question\", \"chain_of_thought\", \"answer\", \"thought_type\"]\n",
    "    if not isinstance(data_from_model, dict) or not all(key in data_from_model for key in required_keys):\n",
    "        tqdm.write(f\"‼️ [WARN] В ответе модели отсутствуют необходимые ключи или некорректный формат для {strategy_type}. Пропускаю. Ответ: {response_content[:100]}...\")\n",
    "        return None\n",
    "\n",
    "    data_from_model['thought_type'] = data_from_model.get('thought_type', strategy_type)\n",
    "\n",
    "    assistant_content = (\n",
    "        f\"<thought_type>{data_from_model['thought_type']}</thought_type>\\n\"\n",
    "        f\"<think>\\n{data_from_model['chain_of_thought']}\\n</think>\\n\" \n",
    "        f\"{data_from_model['answer']}\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": data_from_model[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# --- ОСНОВНОЙ КОД ---\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Основная функция, координирующая процесс генерации датасета\n",
    "    для дистилляции Adaptive Thinking и Skip-Thinking.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        get_client() # Инициализация первого клиента\n",
    "    except ValueError as e:\n",
    "        print(f\"❌ [ОШИБКА] {e}\")\n",
    "        return\n",
    "\n",
    "    # --- ЭТАП 1: Генерация уникальных вопросов ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"🚀 ЭТАП 1: Генерация {NUM_UNIQUE_QUESTIONS_TO_GENERATE} уникальных тем вопросов с помощью Cerebras...\")\n",
    "    all_unique_questions = set() \n",
    "    \n",
    "    with tqdm(total=NUM_UNIQUE_QUESTIONS_TO_GENERATE, desc=\"Генерация уникальных тем вопросов\") as pbar:\n",
    "        while len(all_unique_questions) < NUM_UNIQUE_QUESTIONS_TO_GENERATE and current_key_index < len(API_KEYS):\n",
    "            needed = NUM_UNIQUE_QUESTIONS_TO_GENERATE - len(all_unique_questions)\n",
    "            batch_size = min(needed, QUESTIONS_BATCH_SIZE)\n",
    "            \n",
    "            new_questions_batch = generate_questions_batch(batch_size, random.choice(QUESTION_TOPICS))\n",
    "            \n",
    "            if not new_questions_batch:\n",
    "                if current_key_index >= len(API_KEYS) - 1: # Проверяем, если это последний ключ\n",
    "                    tqdm.write(\"❌ [ERROR] Не удалось сгенерировать вопросы, и все ключи исчерпаны. Остановка.\")\n",
    "                    break\n",
    "                # Если просто не было возвращено вопросов, но ключи еще есть, делаем паузу и продолжаем\n",
    "                time.sleep(RETRY_DELAY) \n",
    "                continue\n",
    "\n",
    "            prev_count = len(all_unique_questions)\n",
    "            all_unique_questions.update(new_questions_batch) \n",
    "            pbar.update(len(all_unique_questions) - prev_count) \n",
    "            time.sleep(0.5) \n",
    "\n",
    "    unique_questions_list = list(all_unique_questions)\n",
    "    print(f\"✅ Сгенерировано {len(unique_questions_list)} уникальных тем вопросов.\")\n",
    "\n",
    "    if not unique_questions_list:\n",
    "        print(\"Вопросы не были сгенерированы. Завершение работы.\")\n",
    "        return\n",
    "\n",
    "    # --- ЭТАП 2: Подготовка задач для параллельной генерации различных стратегий рассуждения ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"🧠 ЭТАП 2: Подготовка задач для генерации рассуждений различных типов...\")\n",
    "    \n",
    "    tasks_for_reasoning = []\n",
    "    strategy_types_list = list(REASONING_STRATEGIES.keys())\n",
    "    \n",
    "    for question in unique_questions_list:\n",
    "        for i in range(NUM_REASONING_STRATEGIES_PER_QUESTION):\n",
    "            # Случайный выбор стратегии. Можно задать веса, если нужно больше определенных типов.\n",
    "            # Например, SkipThinking и CoT могут быть более приоритетными.\n",
    "            chosen_strategy_type = random.choices(\n",
    "                strategy_types_list, \n",
    "                weights=[0.2, 0.5, 0.3] if \"SkipThinking\" in strategy_types_list else [0.5, 0.5], # Пример весов\n",
    "                k=1\n",
    "            )[0]\n",
    "            chosen_prompt_template = REASONING_STRATEGIES[chosen_strategy_type]\n",
    "            \n",
    "            tasks_for_reasoning.append({\n",
    "                \"question\": question,\n",
    "                \"strategy_type\": chosen_strategy_type,\n",
    "                \"prompt_template\": chosen_prompt_template\n",
    "            })\n",
    "    random.shuffle(tasks_for_reasoning) \n",
    "\n",
    "    print(f\"� Сформировано {len(tasks_for_reasoning)} задач на генерацию рассуждений.\")\n",
    "    \n",
    "    # --- ЭТАП 3: Параллельная генерация рассуждений и ответов ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"🚀 ЭТАП 3: Генерация рассуждений (различных типов) в {MAX_WORKERS} потоков...\")\n",
    "    \n",
    "    final_samples = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_task = {\n",
    "            executor.submit(generate_reasoning_example, t['question'], t['strategy_type'], t['prompt_template']): t\n",
    "            for t in tasks_for_reasoning\n",
    "        }\n",
    "        \n",
    "        progress_bar = tqdm(as_completed(future_to_task), total=len(tasks_for_reasoning), desc=\"Генерация рассуждений и ответов\")\n",
    "        \n",
    "        for future in progress_bar:\n",
    "            original_task = future_to_task[future]\n",
    "            try:\n",
    "                result_data = future.result()\n",
    "                if result_data:\n",
    "                    final_samples.append(result_data)\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"❌ [ERROR] Ошибка в потоке при обработке задачи для вопроса '{original_task['question'][:50]}...' ({original_task['strategy_type']}). Ошибка: {e}\")\n",
    "\n",
    "    # --- ЭТАП 4: Запись сгенерированных примеров в файл ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"💾 ЭТАП 4: Запись {len(final_samples)} сгенерированных примеров в файл '{OUTPUT_DATASET_FILE}'...\")\n",
    "    \n",
    "    with open(OUTPUT_DATASET_FILE, 'a', encoding='utf-8') as f:\n",
    "        for entry in final_samples:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    # --- Завершение ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎉 Генерация завершена!\")\n",
    "    print(f\"Датасет сохранен в файл: {OUTPUT_DATASET_FILE}\")\n",
    "    print(f\"Всего сгенерировано {len(final_samples)} валидных примеров для дистилляции Adaptive Thinking.\")\n",
    "    if final_samples:\n",
    "        print(\"\\nПример структуры одной записи в файле:\")\n",
    "        print(json.dumps(final_samples[0], indent=2, ensure_ascii=False)) \n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3677f",
   "metadata": {},
   "source": [
    "Гистограмма длины sample-ов по токенам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d25f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- НАСТРОЙКИ ---\n",
    "# ==============================================================================\n",
    "# Имя файла датасета, сгенерированного предыдущим скриптом\n",
    "OUTPUT_DATASET_FILE = \"D:\\Work\\Python\\Parser_for_distil_dataset\\qwen3_adaptive_reasoning_dataset.jsonl\"\n",
    "# Имя файла для сохранения гистограммы\n",
    "HISTOGRAM_OUTPUT_FILE = \"token_length_histogram.png\"\n",
    "\n",
    "# --- ФУНКЦИЯ ТОКЕНИЗАЦИИ ---\n",
    "# ==============================================================================\n",
    "# Попытка импортировать tiktoken для более точного подсчета токенов.\n",
    "# Если tiktoken не установлен, используется простой подсчет слов.\n",
    "try:\n",
    "    import tiktoken\n",
    "    ENCODER = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    def tokenize_text(text: str) -> int:\n",
    "        \"\"\"Подсчитывает количество токенов в тексте с помощью tiktoken.\"\"\"\n",
    "        return len(ENCODER.encode(text))\n",
    "    logger.info(\"Используется tiktoken для токенизации.\")\n",
    "except ImportError:\n",
    "    logger.warning(\"Библиотека 'tiktoken' не найдена. Используется простой подсчет слов для оценки токенов.\")\n",
    "    def tokenize_text(text: str) -> int:\n",
    "        \"\"\"Подсчитывает количество слов в тексте как прокси для токенов.\"\"\"\n",
    "        return len(text.split())\n",
    "    \n",
    "# --- ОСНОВНАЯ ЛОГИКА ---\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_token_length_histogram():\n",
    "    \"\"\"\n",
    "    Генерирует гистограмму распределения длины токенов для сгенерированных примеров.\n",
    "    Читает данные из OUTPUT_DATASET_FILE, подсчитывает токены и сохраняет гистограмму.\n",
    "    \"\"\"\n",
    "    token_lengths = []\n",
    "\n",
    "    # Проверяем существование файла датасета\n",
    "    if not os.path.exists(OUTPUT_DATASET_FILE):\n",
    "        logger.error(f\"Файл датасета '{OUTPUT_DATASET_FILE}' не найден. Пожалуйста, убедитесь, что он существует и был сгенерирован.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Загрузка данных из '{OUTPUT_DATASET_FILE}' для анализа длины токенов...\")\n",
    "    try:\n",
    "        with open(OUTPUT_DATASET_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "                    # Ожидаем структуру {\"messages\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "                    if \"messages\" in entry and isinstance(entry[\"messages\"], list) and len(entry[\"messages\"]) == 2:\n",
    "                        user_content = entry[\"messages\"][0].get(\"content\", \"\")\n",
    "                        assistant_content = entry[\"messages\"][1].get(\"content\", \"\")\n",
    "                        \n",
    "                        # Объединяем пользовательский и ассистентский контент для подсчета общей длины\n",
    "                        full_text = user_content + \" \" + assistant_content\n",
    "                        token_count = tokenize_text(full_text)\n",
    "                        token_lengths.append(token_count)\n",
    "                    else:\n",
    "                        logger.warning(f\"Неожиданный формат записи в файле: {line.strip()}. Пропускаю.\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logger.warning(f\"Ошибка декодирования JSON в строке: {line.strip()}. Пропускаю. Ошибка: {e}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Неожиданная ошибка при обработке строки: {line.strip()}. Пропускаю. Ошибка: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Ошибка при чтении файла '{OUTPUT_DATASET_FILE}': {e}\")\n",
    "        return\n",
    "\n",
    "    if not token_lengths:\n",
    "        logger.warning(\"Не удалось извлечь длины токенов. Возможно, файл пуст или имеет неверный формат после фильтрации.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Собрано {len(token_lengths)} длин токенов. Создание гистограммы...\")\n",
    "\n",
    "    # Создание гистограммы\n",
    "    plt.figure(figsize=(12, 7)) # Устанавливаем размер фигуры для лучшей читаемости\n",
    "    \n",
    "    num_bins = min(50, int(np.sqrt(len(token_lengths)))) if len(token_lengths) > 0 else 10\n",
    "    \n",
    "    plt.hist(token_lengths, bins=num_bins, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    \n",
    "    # Добавляем заголовок и подписи осей\n",
    "    plt.title('Распределение длины примеров в токенах', fontsize=16)\n",
    "    plt.xlabel('Длина в токенах', fontsize=12)\n",
    "    plt.ylabel('Количество примеров', fontsize=12)\n",
    "    \n",
    "    # Добавляем сетку для лучшей читаемости\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    \n",
    "    # Добавляем линии для среднего и медианы\n",
    "    mean_length = np.mean(token_lengths)\n",
    "    median_length = np.median(token_lengths)\n",
    "    plt.axvline(mean_length, color='r', linestyle='dashed', linewidth=1.5, label=f'Среднее: {mean_length:.2f}')\n",
    "    plt.axvline(median_length, color='g', linestyle='dashed', linewidth=1.5, label=f'Медиана: {median_length:.2f}')\n",
    "    plt.legend() # Отображаем легенду для линий среднего и медианы\n",
    "\n",
    "    # Оптимизируем расположение элементов на графике\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Сохраняем гистограмму в файл\n",
    "    plt.savefig(HISTOGRAM_OUTPUT_FILE)\n",
    "    logger.info(f\"Гистограмма сохранена в '{HISTOGRAM_OUTPUT_FILE}'\")\n",
    "    logger.info(\"Процесс анализа длины токенов завершен.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_token_length_histogram()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc3d688",
   "metadata": {},
   "source": [
    "Выгрузка датасета на Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b793c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"Ваш токен\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049836e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Путь к файлу, созданному вашим скриптом\n",
    "dataset_file_path = \"qwen3_adaptive_reasoning_dataset.jsonl\"\n",
    "\n",
    "# Загрузка JSONL файла\n",
    "my_dataset = load_dataset('json', data_files=dataset_file_path, split=\"train\")\n",
    "\n",
    "# Укажите свой username на Hugging Face и желаемое имя для датасета\n",
    "repo_id = \"ваш_username/qwen3_adaptive_reasoning_dataset\"\n",
    "\n",
    "# Загрузка на Hugging Face Hub\n",
    "my_dataset.push_to_hub(repo_id)\n",
    "\n",
    "print(f\"Датасет успешно загружен на Hugging Face Hub по адресу: https://huggingface.co/datasets/{repo_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
